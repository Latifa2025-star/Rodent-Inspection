# -*- coding: utf-8 -*-
"""NYC DOHMH Rodent Inspection Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KehX9KWZqNw-4yHBtVGdj7mbYNOfObPg

**CELL 1: Load the Full Dataset**

# üêÄ DOHMH Rodent Inspection Dataset: Phase 0: Data Loading

In this first step, I load the **complete DOHMH Rodent Inspection dataset** from NYC Open Data.
This dataset contains verified rodent inspections conducted by the NYC Department of Health, with
features such as inspection type, inspection date, result, geographic information (borough, census tract),
and GPS coordinates.

Loading the full dataset allows me to:
- Understand its size and structure
- Preview the type of information available
- Prepare for cleaning and exploratory analysis

Below, I load the entire dataset (2.8M+ rows) and display its dimensions and first few rows.
"""

import pandas as pd
url = "https://data.cityofnewyork.us/resource/p937-wjvj.csv?$limit=10000"
rodents = pd.read_csv(url)
rodents.to_csv("rodent_sample.csv", index=False)
print("Dataset sample shape:", rodents.shape)
rodents.head()

import pandas as pd

# Load the complete DOHMH Rodent Inspection dataset (no row limit)
url = "https://data.cityofnewyork.us/api/views/p937-wjvj/rows.csv?accessType=DOWNLOAD"
rodents = pd.read_csv(url)

# Show shape and first 5 rows
print("Dataset shape (rows, columns):", rodents.shape)
rodents.head()

"""### Explanation of Output

The dataset successfully loaded, and this output tells me the following:

#### **1. Dataset Size**

This means the dataset contains:
- **2,876,465 rows** ‚Üí almost **3 million inspection records**, which is excellent for large-scale analysis and modeling.
- **25 columns** ‚Üí each record includes many attributes such as location, date, inspection result, and geographic identifiers.

This confirms that the dataset is **large enough, rich enough, and detailed enough** for a strong data story, meaningful visualizations, and advanced machine learning tasks.

---

#### **2. First Five Rows Preview**

The preview shows important columns like:

- **INSPECTION_TYPE** ‚Äì ‚ÄúInitial‚Äù or ‚ÄúRe-inspection‚Äù
- **BOROUGH** ‚Äì Bronx, Manhattan, Brooklyn, etc.
- **INSPECTION_DATE** ‚Äì when the inspection happened
- **RESULT** ‚Äì ‚ÄúPassed‚Äù, ‚ÄúRat Activity‚Äù, etc.
- **LOCATION** ‚Äì GPS coordinates (latitude/longitude)
- **CENSUS TRACT**, **COMMUNITY BOARD**, **NTA** ‚Äì geographic units for mapping and spatial analysis

This confirms that the dataset includes **time information**, **location information**, **neighborhood-level identifiers**, and **inspection results**. These are exactly the types of features needed for:

- Time-series patterns  
- Geographic heatmaps  
- Borough comparisons  
- Predictive modeling  
- Interactive dashboards  

Overall, this dataset is ideal for building a powerful and visually compelling data story.

**Step 2: Inspect Data Types and Missing Values**

Before cleaning or modeling, I need to understand:
- What type of data each column contains (string, number, date, etc.)
- Which columns have missing values and how many
- How consistent the dataset is

This step will help me plan the preprocessing steps needed for analysis, visualization, and modeling.
"""

# Check data types and basic structure
rodents.info()

# Check missing values sorted from most to least
rodents.isnull().sum().sort_values(ascending=False)

"""### Explanation of Output (Cell 2)

This step gives an overview of the dataset‚Äôs structure and missing values so I can plan the cleaning process.



## 1. Data Types (rodents.info())

The dataset contains:
- **10 object columns** ‚Üí text fields (e.g., borough, street name, inspection type)
- **5 integer columns** ‚Üí IDs, codes
- **10 float columns** ‚Üí numeric values that include missing entries (e.g., census tract, community board)

The dataset uses **548 MB of memory**, which confirms it is a **large, complex dataset** suitable for advanced analysis and machine learning.

This also tells me which columns might need:
- Type conversion (e.g., dates from string ‚Üí datetime)
- Cleaning or filling missing values
- Encoding for modeling (categorical ‚Üí numeric)



## 2. Missing Values Overview

Some columns have **no missing values**, such as:
- `LOT`, `BORO_CODE`, `BLOCK`,
- `INSPECTION_TYPE`, `JOB_ID`, `JOB_PROGRESS`

These columns are reliable and complete.

Other columns have significant missing values:
- `Y_COORD` and `X_COORD`: ~845,762 missing  
- `BIN` and `BBL`: ~94,000 missing  
- `COMMUNITY BOARD`, `COUNCIL DISTRICT`, `CENSUS TRACT`, `NTA`: ~75,000 missing  
- `LOCATION`, `LATITUDE`, `LONGITUDE`: ~20,000 missing

This tells me that **geospatial analysis will require handling missing coordinates**.

Columns with very few missing values:
- `BOROUGH`: only 270 missing  
- `RESULT`: 37 missing  
- `INSPECTION_DATE`: 17 missing  

These are easy to fix or drop.



## What this means for preprocessing

- I will convert date columns (`INSPECTION_DATE`, `APPROVED_DATE`) into datetime objects.
- Missing coordinates can be handled by:
  - Dropping rows with missing LAT/LONG for map visualizations, or  
  - Keeping them for other types of analysis.
- Categoricals (e.g., BOROUGH, RESULT, INSPECTION_TYPE) will later be encoded for modeling.
- Some columns like `X_COORD` and `Y_COORD` might be optional if LAT/LONG is available.

This step helps establish a clear cleaning plan before starting exploratory analysis and modeling.

##** Step 3: Summary Statistics for Numerical Columns**

To better understand the numerical variables in the dataset, I will compute summary statistics such as
mean, minimum, maximum, and quartiles using `describe()`.

This will help identify:
- Outliers
- Numerical ranges
- Potential errors
- Useful features for modeling
"""

# Summary statistics for numerical columns
rodents.describe()

"""### Explanation of Output (Cell 3)

This table shows summary statistics for all the numerical columns in the dataset. This helps me understand
the ranges, typical values, and any unusual patterns before starting analysis or modeling.



## 1. Row Counts (the ‚Äúcount‚Äù column)
The `count` values show how many non-missing values each numerical column contains.

Examples:
- `JOB_TICKET_OR_WORK_ORDER_ID` has **2.87 million non-missing values** ‚Üí almost complete.
- `X_COORD`, `Y_COORD` have **only ~2.03 million values** ‚Üí many missing coordinates.
- `LATITUDE`, `LONGITUDE` each have **~2.85 million values** ‚Üí these are more complete and better for mapping.

This tells me which numeric features are reliable and which require cleaning.



## 2. Distribution of Values (mean, min, 25%, 50%, 75%, max)

### **ZIP_CODE**
- Minimum ZIP is **0**, which is invalid.
- ZIP codes range mostly around **10000‚Äì11300**, matching NYC ZIPs.
I may need to remove or filter out invalid ZIP codes.



### **Coordinates (LATITUDE & LONGITUDE)**
- LATITUDE ranges ~**40.5¬∞** (typical latitude for NYC).
- LONGITUDE ranges ~**‚Äì73.9¬∞** (correct for NYC).
- Minimum LONGITUDE is **0** (invalid), so some rows need cleaning.

Latitude/longitude data looks correct overall but contains a few invalid values.



### **BLOCK, LOT, BORO_CODE**
These match NYC property and borough codes:
- `BORO_CODE`: 1‚Äì5 represent Manhattan, Bronx, Brooklyn, Queens, Staten Island.
- `BLOCK` ranges from 0 to 33,325 ‚Üí valid for NYC parcel maps.

These can be used for neighborhood-level or borough-level analysis.



### **INSPECTION_COUNT Columns (COMMUNITY BOARD, CENSUS TRACT, BIN)**
These represent NYC geographic identifiers:
- `CENSUS TRACT` has values up to **157,903**, because census tracts are coded numerically.
- `COMMUNITY BOARD` ranges 1‚Äì18.
- `BIN` (building ID) ranges from 1,000,000 to 5,799,514.

These columns will be very useful for spatial modeling and mapping.



## 3. Detecting Issues & Outliers

These statistics reveal a few issues:
- Some ZIP codes, coordinates, and blocks have **0**, meaning missing or invalid entries.
- Some columns (like LOT and COMMUNITY BOARD) have wide ranges, so I might consider normalizing or transforming them later during modeling.

 This tells me what needs to be cleaned or filtered before building visuals or models.



## Summary
These summary statistics help me understand:
- Which columns have valid numerical ranges
- Which columns have obvious invalid values (0, negative values)
- How I should clean and preprocess data before analysis
- Which features might be most useful for machine learning

This step prepares the dataset for deeper exploratory analysis in the next cells.

**Step 4: Explore Key Categorical Columns**

To understand the structure of the dataset, I will check how many unique values exist in important
categorical columns such as BOROUGH, INSPECTION_TYPE, and RESULT.

This helps me identify:
- Categories to include in visualizations
- Variables to encode for modeling
- Values that may need cleaning or merging
"""

# Check unique values in key categorical columns
categorical_cols = ['BOROUGH', 'INSPECTION_TYPE', 'RESULT', 'NTA']

for col in categorical_cols:
    print(f"\nUnique values in {col}:")
    print(rodents[col].unique())
    print(f"Total unique: {rodents[col].nunique()}")

"""**### Explanation (Cell 4)**

- **BOROUGH**: 5 valid boroughs + a few missing values. Useful for comparing rodent activity across NYC.
- **INSPECTION_TYPE**: 3 types (‚ÄúInitial‚Äù, ‚ÄúBAIT‚Äù, ‚ÄúCompliance‚Äù). Helps analyze inspection workflows.
- **RESULT**: 5 main outcomes (e.g., ‚ÄúPassed‚Äù, ‚ÄúRat Activity‚Äù). This will be important for prediction models.
- **NTA**: 247 neighborhoods. Very detailed geographic data for maps and hotspot analysis.

Overall, these categorical columns give strong support for visualizations, filtering, trend analysis, and machine learning models.

**Step 5: Convert Date Columns and Extract Year/Month**

The date columns are currently stored as text.
Here, I convert INSPECTION_DATE and APPROVED_DATE to proper datetime format,
then extract the inspection year and month for time-based analysis.
"""

# Convert date columns to datetime
rodents['INSPECTION_DATE'] = pd.to_datetime(rodents['INSPECTION_DATE'], errors='coerce')
rodents['APPROVED_DATE'] = pd.to_datetime(rodents['APPROVED_DATE'], errors='coerce')

# Extract year and month
rodents['INSPECTION_YEAR'] = rodents['INSPECTION_DATE'].dt.year
rodents['INSPECTION_MONTH'] = rodents['INSPECTION_DATE'].dt.month

# Show results
rodents[['INSPECTION_DATE', 'INSPECTION_YEAR', 'INSPECTION_MONTH']].head()

"""**Explanation (Cell 5)**

The date columns were successfully converted to proper datetime format.  
The new columns `INSPECTION_YEAR` and `INSPECTION_MONTH` were created correctly.

Example:
- 2010-08-30 ‚Üí Year = 2010, Month = 8  
- 2018-10-10 ‚Üí Year = 2018, Month = 10  

This will allow me to analyze trends over time (yearly, monthly, seasonal patterns).  
The warning is normal ‚Äî it just means pandas had to auto-detect the date format.

**Step 6: Inspect the Time Range of Inspections**

Now that the dates are properly converted, I will check the earliest and latest inspection dates.
This helps me understand the full time span of the dataset, which is important for describing the data
in my proposal and for planning time-based analysis.
"""

# Find earliest and latest inspection dates
earliest = rodents['INSPECTION_DATE'].min()
latest = rodents['INSPECTION_DATE'].max()

earliest, latest

"""**Explanation (Cell 6)**

The earliest date shows **1918**, and the latest shows **2045**, which is not realistic.  
These extreme values are data errors.

However, almost all valid inspection records fall between **2010 and 2024**, based on the earlier preview.

This tells me:
- The dataset contains some incorrect dates that I may need to filter out later.
- The real analysis should focus on the modern inspection years (2010‚Äìpresent).

This step confirms the dataset spans many years but includes a few invalid outliers.

**Step 7: Remove Invalid Dates and Get the True Time Range**

The previous step showed some unrealistic dates (e.g., 1918 and 2045).  
Here, I filter the dataset to keep only reasonable inspection years (2010‚Äì2024),
which represent the true period of modern NYC rodent inspections.

Then I compute the real earliest and latest inspection dates.
"""

# Keep only realistic inspection years (2010‚Äì2024)
rodents_clean = rodents[(rodents['INSPECTION_YEAR'] >= 2010) & (rodents['INSPECTION_YEAR'] <= 2024)]

# Find the true time range
true_earliest = rodents_clean['INSPECTION_DATE'].min()
true_latest = rodents_clean['INSPECTION_DATE'].max()

true_earliest, true_latest

"""**Explanation (Cell 7)**

After filtering out unrealistic years, the true inspection dates range from:

- **January 1, 2010**
- **to December 31, 2024**

This confirms that the dataset covers **15 years of rodent inspections** in NYC.

This cleaned time range is accurate and will be used for all analysis, visualizations, and the proposal.

Step 8: Count Inspection Results**

To understand the distribution of outcomes in the dataset, I will count how many inspections fall into
each RESULT category. This helps identify the most common outcomes and is useful for both the data
story and future modeling tasks.

**Step 8: Count Inspection Results**

To understand the distribution of outcomes in the dataset, I will count how many inspections fall into
each RESULT category. This helps identify the most common outcomes and is useful for both the data
story and future modeling tasks.
"""

# Count results of inspections
rodents_clean['RESULT'].value_counts(dropna=False)

"""**Explanation (Cell 8)**

The most common inspection results are:

- **Passed:** 1,615,216  
- **Rat Activity:** 448,281  
- **Bait applied:** 374,984  
- **Failed for Other R:** 233,056  
- **Monitoring visit:** 215  
- **Missing result:** 37  

This shows that most inspections pass, but there is still a large number of cases with confirmed
**rat activity** or **bait applied**, which will be important for analysis and for building a
classification model.

**Step 9: Count Inspections by Borough**

To understand how rodent inspections are distributed across New York City,
I will count the number of inspections in each borough.

This helps identify which boroughs have the highest inspection activity,
which is useful for visualizations and for shaping the project‚Äôs data story.
"""

# Count inspections by borough
rodents_clean['BOROUGH'].value_counts(dropna=False)

"""**Explanation (Cell 9)**

The boroughs with the highest number of rodent inspections are:

- **Manhattan:** 858,442  
- **Brooklyn:** 765,588  
- **Bronx:** 760,200  
- **Queens:** 226,515  
- **Staten Island:** 60,784  
- **Missing borough:** 260  

Manhattan, Brooklyn, and the Bronx have the largest inspection volumes, while Queens and
especially Staten Island have fewer inspections.  

This borough distribution will be important for visual comparisons and analyzing
which areas experience the most rodent-related activity.

**Step 10: Count Inspections by Year**

To understand how rodent inspection activity has changed over time,
I will count the number of inspections performed each year.

This helps identify trends such as increases, decreases, or sudden spikes in inspection activity.
"""

# Count inspections per year
rodents_clean['INSPECTION_YEAR'].value_counts().sort_index()

"""**Explanation (Cell 10)**

Inspection activity has changed over time:

- Steady growth from **2010 to 2012**  
- A dip in **2013‚Äì2015**  
- A major rise in **2016‚Äì2019**, with inspections peaking around 2018‚Äì2019  
- A sharp decline in **2020**, likely due to COVID-19  
- A recovery from **2021‚Äì2024**, returning to high levels

This time trend will be very useful for visualizations and for discussing how inspection activity varies
across years in the project.

**Step 11: Count Inspections by Month**

To check for seasonal patterns, I will count inspections for each month (January‚ÄìDecember).
This helps identify whether certain months have more rodent-related activity or inspections.
"""

# Count inspections per month
rodents_clean['INSPECTION_MONTH'].value_counts().sort_index()

"""**Explanation (Cell 11)**

There is a clear seasonal pattern in inspection activity:

- **Highest months:** March, April, May  
- **Lower months:** July, September, October  
- **Winter months (Jan‚ÄìFeb):** Moderate

This suggests that rodent inspections ‚Äî and possibly rodent activity ‚Äî tend to increase
in the spring and early summer. This seasonal trend will be useful for visualizations
and for creating a time-based narrative in the project.

**Step 12: Count Inspections by Neighborhood (NTA)**

Neighborhood Tabulation Areas (NTA) give a detailed view of where inspections happen.  
Counting inspections by neighborhood helps identify high-activity areas and supports
spatial analysis in the project plan.
"""

# Count inspections by neighborhood (NTA)
rodents_clean['NTA'].value_counts(dropna=False).head(20)

"""**Explanation (Cell 12)**

The neighborhoods (NTA) with the highest inspection counts include:

- **Bedford-Stuyvesant (West):** 139,813  
- **Bushwick (West):** 112,803  
- **Bedford-Stuyvesant (East):** 89,207  
- **East Village:** 76,494  
- **Upper West Side (Central):** 60,169  

These neighborhoods have the most rodent inspections, showing where activity and enforcement
are most concentrated. This will be useful for mapping, hotspot analysis, and explaining spatial
patterns in the proposal.

**Step 13: Visualize rodent inspection trends over time (yearly)**


To understand long-term patterns in rodent activity, we examine how inspection counts have changed from 2010 to 2024.  
This visualization helps identify rising or falling trends, seasonal spikes, and any shifts caused by events such as policy changes or the COVID-19 pandemic.
"""

# Count inspections per year
yearly_counts = rodents_clean['INSPECTION_YEAR'].value_counts().sort_index()

# Plot yearly trend
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(yearly_counts.index, yearly_counts.values, marker='o')
plt.title("Rodent Inspections per Year (2010‚Äì2024)")
plt.xlabel("Year")
plt.ylabel("Number of Inspections")
plt.grid(True)
plt.show()

# Display the counts table as well
yearly_counts

"""**Explanation**

The chart shows how rodent inspections changed from 2010 to 2024.
There is a steady increase from 2016 to 2019, reaching the highest inspection counts around 2018‚Äì2019.
In 2020, inspections drop sharply because of COVID-19 restrictions, then rise again starting in 2021 and return to high levels by 2022‚Äì2024.

Overall, the trend shows:

Strong growth in inspections before the pandemic

A major interruption in 2020

A full rebound in the following years

These patterns help us analyze how rodent activity and inspection priorities changed over time.

**## Step 14: Monthly Trends in Rodent Inspections**

Rodent activity often follows seasonal patterns.  
To understand these patterns, we plot the total number of inspections for each month (Jan‚ÄìDec) across all years.  
This helps identify peak months and yearly cyclic behavior.
"""

# Monthly inspection counts
monthly_counts = rodents_clean['INSPECTION_MONTH'].value_counts().sort_index()

# Plot monthly trend
plt.figure(figsize=(12, 6))
plt.plot(monthly_counts.index, monthly_counts.values, marker='o')
plt.title("Rodent Inspections by Month (All Years Combined)")
plt.xlabel("Month")
plt.ylabel("Number of Inspections")
plt.xticks(range(1,13))
plt.grid(True)
plt.show()

monthly_counts

"""The plot shows strong seasonality in rodent inspections across NYC.
Inspections rise sharply in early spring, peak in March and April, and slowly decline through the summer and fall.
The lowest inspection levels appear in January, likely due to colder weather reducing rodent activity and limiting outdoor inspections.

This seasonal pattern confirms that rodent activity‚Äîand the city‚Äôs inspection responses‚Äîfollow predictable yearly cycles.
These findings will support time-series visualizations, forecasting, and explanations in the project report.

**Step 15: Heatmap of Inspections by Year and Month**

A heatmap helps visualize how rodent inspection activity changes across both years and months.
This makes seasonal patterns, yearly trends, and anomalies (such as the COVID-19 decline) easier to detect.
"""

import seaborn as sns

# Create a pivot table of inspections by Year and Month
heatmap_data = rodents_clean.pivot_table(
    index='INSPECTION_YEAR',
    columns='INSPECTION_MONTH',
    values='JOB_ID',
    aggfunc='count'
)

# Plot the heatmap
plt.figure(figsize=(14, 8))
sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=0.5)
plt.title("Heatmap of Rodent Inspections by Year and Month")
plt.xlabel("Month")
plt.ylabel("Year")
plt.show()

heatmap_data

"""**Explanation**

The heatmap shows clear seasonal and yearly patterns in rodent inspections.
Spring months, especially March, April, and May‚Äîhave consistently higher inspection counts across most years, while winter months show lower activity.

There is a sharp drop across all months in 2020, reflecting the COVID-19 shutdown.
Inspections rise again in 2021‚Äì2024, returning to pre-pandemic levels.

This visualization confirms strong seasonality and highlights years with unusual activity, supporting both the exploratory analysis and the time-based modeling planned for the project.

**Step 16: Validate and Clean Latitude/Longitude Coordinates**

To prepare for geographic visualizations, we need to ensure all latitude and longitude values are valid.
This step removes rows with missing or impossible coordinates so they can be plotted accurately on maps.
"""

# Check valid coordinate ranges
# Latitude range for NYC ~ 40.4 to 40.95
# Longitude range for NYC ~ -74.3 to -73.6

# Create a mask for valid coordinates
valid_coords = (
    rodents_clean['LATITUDE'].between(40.4, 40.95) &
    rodents_clean['LONGITUDE'].between(-74.3, -73.6)
)

# Count invalid rows BEFORE cleaning
invalid_before = (~valid_coords).sum()

# Filter dataset
rodents_geo = rodents_clean[valid_coords].copy()

# Count invalid rows AFTER cleaning
invalid_after = rodents_geo['LATITUDE'].isna().sum() + rodents_geo['LONGITUDE'].isna().sum()

invalid_before, rodents_geo.shape

"""About 33,560 rows contained invalid or out-of-range coordinates.
After removing those entries, the geospatial dataset now contains 2,638,229 inspections with valid latitude and longitude values.

This ensures that all geographic visualizations, such as maps by borough, neighborhood, or hotspot density, will be accurate and reliable.

**## Step 17: Inspections by Borough (Geospatial-Cleaned Data)**

Now that invalid coordinates have been removed, we compute the number of inspections per borough.
This will support comparative spatial analysis, identify high-activity areas, and provide inputs for the dashboard and modeling.
"""

# Count inspections by borough using the cleaned geospatial dataset
borough_counts_geo = rodents_geo['BOROUGH'].value_counts(dropna=False)
borough_counts_geo

"""**Explanation**

After filtering out invalid coordinates, Manhattan, Brooklyn, and the Bronx have the highest number of rodent inspections. Queens and Staten Island show much lower activity.
This confirms a strong spatial pattern: rodent enforcement is most concentrated in the city‚Äôs denser and older housing areas.
These counts will support maps, comparisons, and the borough-level sections of the analysis.

**Step 18: Bar Chart of Inspections by Borough**

This bar chart visualizes inspection volume across NYC boroughs using the cleaned geospatial dataset.
It highlights which areas receive the most rodent inspections and supports spatial analysis and dashboard visuals.
"""

plt.figure(figsize=(10, 6))
borough_counts_geo.sort_values(ascending=False).plot(kind='bar', color='skyblue', edgecolor='black')

plt.title("Rodent Inspections by Borough (Cleaned Geo Data)")
plt.ylabel("Number of Inspections")
plt.xlabel("Borough")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.show()

"""**Explanation**

The bar chart shows that Manhattan, Brooklyn, and the Bronx have the highest number of rodent inspections, with Manhattan slightly leading. Queens has far fewer inspections, and Staten Island has the lowest.
This reflects differences in population density, building age, and housing conditions across boroughs.
These borough-wide patterns will support your spatial analysis, dashboard filters, and modeling features.

**Step 19: Inspections by Neighborhood (NTA) for Mapping**

Neighborhood Tabulation Areas (NTAs) give a detailed spatial view of rodent inspection activity.
This step computes inspection counts per neighborhood using the cleaned geospatial dataset.
This will later be used for interactive maps and hotspot visualizations.
"""

# Count inspections per neighborhood using geo-clean dataset
nta_counts_geo = rodents_geo['NTA'].value_counts(dropna=False)

# Show top 20 neighborhoods
nta_counts_geo.head(20)

# Fancy styling + imports
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# Global plotly style
px.defaults.template = "plotly_white"
px.defaults.width = 1000
px.defaults.height = 520

# Helper color palettes (choose what you like later)
PAL_SEQ = px.colors.sequential.Agsunset   # warm & classy
PAL_SEQ_ALT = px.colors.sequential.Viridis
PAL_DIV = px.colors.diverging.Spectral
PAL_QUAL = px.colors.qualitative.Safe

"""**Rodent Inspections per Year (2010‚Äì2024)**"""

# Compute yearly counts
year_counts = (
    rodents_clean.dropna(subset=['INSPECTION_YEAR'])
    .groupby('INSPECTION_YEAR')
    .size()
    .reset_index(name='count')
    .sort_values('INSPECTION_YEAR')
)

# Extract the 2020 inspection count
y2020 = year_counts.loc[year_counts['INSPECTION_YEAR'] == 2020, 'count'].values[0]

fig = px.line(
    year_counts, x='INSPECTION_YEAR', y='count',
    markers=True, color_discrete_sequence=[PAL_SEQ[2]],
    title="Rodent Inspections per Year (2010‚Äì2024)"
)

fig.update_traces(line=dict(width=4))

# Add clean annotation for COVID dip
fig.add_annotation(
    x=2020,
    y=y2020 - 15000,       # place label below the actual 2020 point
    text="<b>COVID-19 dip</b>",
    showarrow=True,
    arrowhead=3,
    arrowsize=1.2,
    arrowwidth=2,
    arrowcolor="purple",
    font=dict(size=14, color="purple"),
    ax=0,
    ay=40
)

fig.show()

"""Explanation (Rodent Inspections per Year ‚Äì 2010 to 2024)

Rodent inspections were steady from 2010 to 2016, then rose sharply and reached their highest levels around 2017‚Äì2019. In 2020, there is a clear and dramatic drop due to COVID-19 shutdowns, when city operations slowed. After 2020, inspections quickly recovered and returned to pre-pandemic levels by 2022‚Äì2024.

**Seasonality: Inspections by Month (All Years)**
"""

import plotly.express as px

# Build month counts (you already have this)
month_counts = (
    rodents_clean.dropna(subset=['INSPECTION_MONTH'])
    .groupby('INSPECTION_MONTH')
    .size()
    .reset_index(name='count')
    .sort_values('INSPECTION_MONTH')
)

# Identify the top 2 months by inspection count
top2_months = month_counts.nlargest(2, 'count')['INSPECTION_MONTH'].tolist()
peak_label = "Peak months (" + ", ".join(str(int(m)) for m in sorted(top2_months)) + ")"

# Create a highlight column: top 2 vs others
month_counts['highlight'] = month_counts['INSPECTION_MONTH'].apply(
    lambda m: peak_label if m in top2_months else 'Other months'
)

# Fancy, presentation-ready colors
colors = {
    'Other months': '#D8DCEF',          # light desaturated
    peak_label:   '#C2185B'             # bold magenta (stands out on slides)
}

fig = px.bar(
    month_counts,
    x='INSPECTION_MONTH', y='count',
    color='highlight', color_discrete_map=colors,
    title="Rodent Inspections by Month (All Years Combined)"
)

# Style for slides
fig.update_traces(marker_line_color='white', marker_line_width=1.2, opacity=0.95)
fig.update_layout(
    xaxis_title="Month",
    yaxis_title="Number of Inspections",
    legend_title="",
    bargap=0.2,
    template="simple_white"
)
fig.update_yaxes(tickformat=',')  # 200,000 style ticks

# Add subtle annotations on the peak bars
for m in top2_months:
    val = month_counts.loc[month_counts['INSPECTION_MONTH'] == m, 'count'].iloc[0]
    fig.add_annotation(
        x=m, y=val,
        text="Peak",
        showarrow=True, arrowhead=2, arrowsize=1, arrowwidth=1.2, arrowcolor='#C2185B',
        yshift=12, font=dict(color='#C2185B', size=12)
    )

fig.show()

"""Explanation (Rodent Inspections by Month ‚Äì All Years Combined)
Rodent inspections peak sharply in March and April, the two highest-activity months of the year. These months likely reflect seasonal factors such as warmer weather, increased rodent movement, and intensified city inspections. Activity remains high through summer, then gradually declines toward winter. This pattern shows a clear seasonal cycle, where spring and early summer bring the most rodent-related activity citywide.

**Outcome Mix (Donut)**
"""

# --- Outcome Mix Donut Chart (Full Working Cell) ---

# Fix column names cleanly using reset_index(name='count')
result_counts = (
    rodents_clean['RESULT']
    .fillna('Unknown')
    .value_counts()
    .reset_index(name='count')
    .rename(columns={'index': 'RESULT'})
)

# Donut Chart
fig = px.pie(
    result_counts,
    names='RESULT',
    values='count',
    hole=0.45,
    color='RESULT',
    color_discrete_sequence=PAL_QUAL,   # your fancy qualitative palette
    title="Inspection Outcomes Mix (2010‚Äì2024)"
)

# Make labels clean and readable
fig.update_traces(
    textposition='inside',
    textinfo='percent+label',
    pull=[0.08 if r == "Rat Activity" else 0 for r in result_counts['RESULT']]  # slight pull for emphasis
)

# Layout refinements
fig.update_layout(
    showlegend=True,
    legend_title_text='Outcome Type',
    font=dict(size=14)
)

fig.show()

# Cell D ‚Äî Outcome Mix (Donut) ‚Äî cleaned, grouped, and nicely labeled

import pandas as pd
import plotly.express as px

# 1) Count outcomes and rename a label for clarity
rc = (
    rodents_clean['RESULT']
    .fillna('Unknown')
    .replace({'Failed for Other R': 'Failed for Other Reason'})
    .value_counts()
    .reset_index()
)
rc.columns = ['RESULT', 'count']

# 2) Group ultra-small categories into "Other (<0.1%)"
total = rc['count'].sum()
rc['share'] = rc['count'] / total
threshold = 0.001  # 0.1%
small_mask = rc['share'] < threshold

if small_mask.any():
    other_count = rc.loc[small_mask, 'count'].sum()
    rc = rc.loc[~small_mask, ['RESULT', 'count']]
    if other_count > 0:
        rc = pd.concat(
            [rc, pd.DataFrame([{'RESULT': 'Other (<0.1%)', 'count': other_count}])],
            ignore_index=True
        )

# 3) Sort for a stable, readable legend
rc = rc.sort_values('count', ascending=False)

# 4) Color map (consistent, professional palette)
color_map = {
    'Passed': '#8BD0F3',
    'Rat Activity': '#CF5F6B',
    'Bait applied': '#E3CF80',
    'Failed for Other Reason': '#1F7A3A',
    'Monitoring visit': '#5B2CA4',
    'Unknown': '#8F8F8F',
    'Other (<0.1%)': '#B0B0B0'
}
present_map = {k: v for k, v in color_map.items() if k in set(rc['RESULT'])}

# 5) Donut chart
fig = px.pie(
    rc, names='RESULT', values='count',
    hole=0.45, color='RESULT', color_discrete_map=present_map,
    title="Inspection Outcomes Mix (2010‚Äì2024)"
)
fig.update_traces(
    textposition='inside',
    texttemplate='%{label}<br>%{percent:.1%}',
    hovertemplate='%{label}: %{value:,} inspections<br>%{percent:.2%} share'
)
fig.update_layout(showlegend=True)
fig.show()

# (Optional) Quick sanity check
pct_sum = (rc['count'] / rc['count'].sum() * 100).sum()
print(f"Percent total (from unrounded values): {pct_sum:.2f}%")

"""Most rodent inspections in NYC passed, meaning no rodent activity was found. This makes up about 60% of all inspections.

The second most common result is Rat Activity (around 17%), where inspectors found signs of rats.
Bait applied comes next (about 14%), meaning pest-control bait was placed even if rats were not clearly seen.

Smaller categories like Failed for Other Reasons and Monitoring visit make up the rest.

Overall, the chart shows that while many inspections pass, a significant number still involve rodent issues or require pest-control action.

**Inspections by Borough (Sorted)**
"""

borough_counts = (
    rodents_clean.dropna(subset=['BOROUGH'])
    .groupby('BOROUGH').size().reset_index(name='count')
    .sort_values('count', ascending=False)
)

fig = px.bar(
    borough_counts, x='BOROUGH', y='count', text='count',
    color='BOROUGH', color_discrete_sequence=PAL_QUAL,
    title="Rodent Inspections by Borough (2010‚Äì2024)"
)
fig.update_traces(texttemplate='%{text:,}', textposition='outside')
fig.update_layout(xaxis_title="", yaxis_title="Inspections", uniformtext_minsize=10, uniformtext_mode='show')
fig.show()

"""**Explanation: Rodent Inspections by Borough (2010‚Äì2024)**

Manhattan, Brooklyn, and the Bronx have the highest number of rodent inspections, each with more than 750,000 inspections. This shows where NYC focuses most of its rodent control efforts.

Queens has far fewer inspections (about 226,000), and Staten Island has the lowest (60,784).
The pattern reflects differences in population density, building age, and the number of complaint requests across boroughs.

**Top 20 Neighborhoods (NTA) ‚Äî Horizontal Bar**
"""

# Top 20 Neighborhoods by inspection count
nta_counts = (
    rodents_clean['NTA']
    .fillna('Unknown')
    .value_counts()
    .head(20)
    .reset_index()
    .rename(columns={'index':'NTA', 0:'count'})
    .sort_values('count', ascending=True)
)

fig = px.bar(
    nta_counts,
    x='count',
    y='NTA',
    orientation='h',
    color='count',
    color_continuous_scale=PAL_SEQ_ALT,
    title="Top 20 Neighborhoods by Inspection Volume"
)

fig.update_layout(
    xaxis_title="Inspections",
    yaxis_title="",
    coloraxis_showscale=False
)

fig.show()

"""**Explanation (Top 20 Neighborhoods by Inspection Volume)**

This chart highlights the 20 NYC neighborhoods with the highest number of rodent inspections from 2010‚Äì2024. A few neighborhoods clearly stand out:

Bedford-Stuyvesant (West) has the highest inspection volume, indicating persistent rodent issues and strong enforcement activity.

Bushwick (West) and Bedford-Stuyvesant (East) follow closely, showing similar high-density rodent activity patterns.

Neighborhoods like East Village, Upper West Side (Central), and Harlem (North/South) also appear frequently, reflecting ongoing challenges in older buildings and high-density housing zones.

The gradual decline in bar lengths further down the chart shows a transition from severe hotspots to moderately affected areas.

Overall, this ranking helps identify NYC‚Äôs rodent hotspots, guiding where targeted interventions, sanitation improvements, or predictive modeling efforts may have the strongest impact.

**Year √ó Month Heatmap (Interactive)**
"""

pivot = (
    rodents_clean
    .pivot_table(index='INSPECTION_YEAR', columns='INSPECTION_MONTH', values='JOB_ID', aggfunc='count')
    .sort_index()
)
fig = px.imshow(
    pivot, aspect='auto', color_continuous_scale=PAL_SEQ,
    labels=dict(color="Inspections"),
    title="Heatmap of Rodent Inspections by Year and Month"
)
fig.update_xaxes(title="Month")
fig.update_yaxes(title="Year")
fig.show()

"""The heatmap shows monthly rodent inspections from 2010‚Äì2024, where lighter colors mean more inspections. A clear seasonal pattern appears: spring and summer consistently have the highest activity, while winter months are lower. The sharp dark patch in 2020 reflects the COVID-19 service slowdown. After 2021, inspections rise again across most months. Overall, the heatmap highlights strong seasonality, a clear pandemic dip, and a long-term upward trend in inspection volume.

**Interactive NYC Map (sampled for speed)**
"""

# Keep rows with valid lat/lon; sample for speed
geo = rodents_clean.dropna(subset=['LATITUDE','LONGITUDE','RESULT'])
if len(geo) > 50000:
    geo = geo.sample(50000, random_state=42)

fig = px.scatter_mapbox(
    geo, lat='LATITUDE', lon='LONGITUDE', hover_name='BOROUGH',
    hover_data={'RESULT':True, 'INSPECTION_DATE':True, 'LATITUDE':False,'LONGITUDE':False},
    color='RESULT', color_discrete_sequence=PAL_QUAL,
    zoom=9, height=650, title="Rodent Inspections Map (sampled)"
)
fig.update_layout(mapbox_style="open-street-map", legend_title_text="Result")
fig.show()

"""Passed
The inspector found no signs of rodents. The location met all health requirements.

Bait applied
The inspector placed rodent bait because conditions suggested potential rodent activity, but no active infestation was confirmed.

Failed for Other R
The inspection failed for reasons other than rodents, such as environmental or structural issues.

Rat Activity
The inspector confirmed rat presence, such as droppings, burrows, tracks, or live rats. This is the main indicator of rodent infestation.

Monitoring visit
A follow-up or routine visit to check previous activity, without performing a full inspection.

**Explanation**

This map shows a sample of rodent inspections plotted across New York City. Each dot represents a single inspection, colored by the result. Large clusters of points appear in Manhattan, Brooklyn, and the Bronx, showing where inspections‚Äîand rodent activity‚Äîare most concentrated. Areas with more green and red dots indicate neighborhoods with more ‚ÄúRat Activity‚Äù findings or bait applications. Overall, the map highlights the geographic hotspots where rodent issues are most common.

**Feature Importance Preview (quick baseline)**
"""

# Binary target: Rat Activity vs Other
dfm = rodents_clean.copy()
dfm['target'] = (dfm['RESULT'] == 'Rat Activity').astype(int)

# Simple feature set
feat_cols = ['BOROUGH','INSPECTION_TYPE','INSPECTION_YEAR','INSPECTION_MONTH']
dfm = dfm.dropna(subset=feat_cols + ['target'])

# One-hot encode
X = pd.get_dummies(dfm[feat_cols], drop_first=True)
y = dfm['target']

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced_subsample')
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)[:,1]
print(classification_report(y_test, y_pred, digits=3))
print("ROC‚ÄìAUC:", roc_auc_score(y_test, y_prob))

# Feature importance plot (interactive)
imp = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_}).sort_values('importance', ascending=False).head(20)
fig = px.bar(imp, x='importance', y='feature', orientation='h',
             color='importance', color_continuous_scale=PAL_SEQ,
             title="Random Forest ‚Äî Top Feature Importances (Preview)")
fig.update_layout(xaxis_title="Importance", yaxis_title="", coloraxis_showscale=False)
fig.show()